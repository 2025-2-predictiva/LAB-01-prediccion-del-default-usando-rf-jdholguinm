{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04266187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfdd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Paso 1: cargar y preprocesar datos\"\n",
    "def cargar_preprocesar_datos():\n",
    "    train_dataset = pd.read_csv(\"../files/input/train_data.csv.zip\", index_col=False)\n",
    "    test_dataset = pd.read_csv(\"../files/input/test_data.csv.zip\", index_col=False)\n",
    "\n",
    "    train_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    test_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "\n",
    "    train_dataset.drop(columns=\"ID\", inplace=True)\n",
    "    test_dataset.drop(columns=\"ID\", inplace=True)\n",
    "\n",
    "    train_dataset = train_dataset[train_dataset[\"EDUCATION\"] != 0]\n",
    "    test_dataset = test_dataset[test_dataset[\"EDUCATION\"] != 0]\n",
    "\n",
    "    train_dataset = train_dataset[train_dataset[\"MARRIAGE\"] != 0]\n",
    "    test_dataset = test_dataset[test_dataset[\"MARRIAGE\"] != 0]\n",
    "\n",
    "    train_dataset[\"EDUCATION\"] = train_dataset[\"EDUCATION\"].apply(lambda x: 4 if x > 4 else x)\n",
    "    test_dataset[\"EDUCATION\"] = test_dataset[\"EDUCATION\"].apply(lambda x: 4 if x > 4 else x)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452b2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Paso 2: División de los datos en conjuntos de entrenamiento y prueba\"\n",
    "def make_train_test_split(train_dataset, test_dataset):\n",
    "    X_train = train_dataset.drop(columns=\"default\")\n",
    "    y_train = train_dataset[\"default\"]\n",
    "\n",
    "    X_test = test_dataset.drop(columns=\"default\")\n",
    "    y_test = test_dataset[\"default\"]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e872e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Paso 3: Crear el Pipeline y preprocesar las variables categóricas usando OneHotEncoder y las numéricas sin cambios\"\n",
    "def make_pipeline():\n",
    "    categorical_features = [\"EDUCATION\", \"MARRIAGE\", \"SEX\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(), categorical_features),\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"rf\", RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359c746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Paso 4: Optimización de los hiperparámetros\"\n",
    "def make_grid_search(pipeline, X_train, y_train):\n",
    "    param_grid = {\n",
    "    \"rf__n_estimators\": [100, 200],\n",
    "    \"rf__max_depth\": [5, 10, None],\n",
    "    \"rf__min_samples_split\": [2, 5],\n",
    "    \"rf__min_samples_leaf\": [1, 2]\n",
    "    \n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv = 10,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e57407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Paso 5: Guardar Modelo\"\n",
    "def save_estimator(estimator):\n",
    "    models_path = \"../files/models\"\n",
    "    os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "    model_file = os.path.join(models_path, \"model.pkl.gz\")\n",
    "\n",
    "    with gzip.open(model_file, \"wb\") as file:\n",
    "        pickle.dump(estimator, file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b674277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Paso 6: Metricas y guardarlas en formato JSON\"\n",
    "# Calcule las metricas de precision, precision balanceada, recall,\n",
    "# y f1-score para los conjuntos de entrenamiento y prueba.\n",
    "# Guardelas en el archivo files/output/metrics.json. Cada fila\n",
    "# del archivo es un diccionario con las metricas de un modelo.\n",
    "# Este diccionario tiene un campo para indicar si es el conjunto\n",
    "# de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'dataset': 'train', 'precision': 0.8, 'balanced_accuracy': 0.7, 'recall': 0.9, 'f1_score': 0.85}\n",
    "# {'dataset': 'test', 'precision': 0.7, 'balanced_accuracy': 0.6, 'recall': 0.8, 'f1_score': 0.75}\n",
    "\n",
    "\n",
    "def calc_metrics(model, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    metricas =[\n",
    "        {\n",
    "            'type': 'metrics',\n",
    "            'dataset': 'train',\n",
    "            'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "            'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_train, y_train_pred, zero_division=0)\n",
    "        },\n",
    "        {\n",
    "            'type': 'metrics',\n",
    "            'dataset': 'test',\n",
    "            'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),\n",
    "            'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_test, y_test_pred, zero_division=0)\n",
    "        },\n",
    "        {\n",
    "            'type': 'cm_matrix',\n",
    "            'dataset': 'train',\n",
    "            'true_0': {'predicted_0': int(cm_train[0, 0]), 'predicted_1': int(cm_train[0, 1])},\n",
    "            'true_1': {'predicted_0': int(cm_train[1, 0]), 'predicted_1': int(cm_train[1, 1])}\n",
    "        },\n",
    "        {\n",
    "            'type': 'cm_matrix',\n",
    "            'dataset': 'test',\n",
    "            'true_0': {'predicted_0': int(cm_test[0, 0]), 'predicted_1': int(cm_test[0, 1])},\n",
    "            'true_1': {'predicted_0': int(cm_test[1, 0]), 'predicted_1': int(cm_test[1, 1])}\n",
    "        }\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "    return metricas\n",
    "\n",
    "\n",
    "\n",
    "def save_metrics(metricas, output_path=\"files/output/metrics.json\"):\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    \"\"\"\n",
    "    Guarda las métricas en formato JSON.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    metricas : lista devuelta por metrics()\n",
    "    output_path : ruta del archivo JSON a guardar\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metricas, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Métricas guardadas en: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0f4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Ejecución del modelo\"\n",
    "\n",
    "def main():\n",
    "    train_dataset, test_dataset = cargar_preprocesar_datos()\n",
    "    X_train, y_train, X_test, y_test = make_train_test_split(train_dataset, test_dataset)\n",
    "    pipeline = make_pipeline()\n",
    "    model = make_grid_search(pipeline, X_train, y_train)\n",
    "    save_estimator(model)\n",
    "    metrics = calc_metrics(model, X_train, y_train, X_test, y_test)\n",
    "    save_metrics(metrics)\n",
    "\n",
    "    print(model.best_estimator_)\n",
    "    print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cab061",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [20953, 8979]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m X_train, y_train, X_test, y_test = make_train_test_split(train_dataset, test_dataset)\n\u001b[32m      6\u001b[39m pipeline = make_pipeline()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mmake_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m save_estimator(model)\n\u001b[32m      9\u001b[39m metrics = calc_metrics(model, X_train, y_train, X_test, y_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mmake_grid_search\u001b[39m\u001b[34m(pipeline, X_train, y_train)\u001b[39m\n\u001b[32m      3\u001b[39m param_grid = {\n\u001b[32m      4\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mrf__n_estimators\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m],\n\u001b[32m      5\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mrf__max_depth\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m grid = GridSearchCV(\n\u001b[32m     12\u001b[39m pipeline,\n\u001b[32m     13\u001b[39m param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m verbose=\u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grid\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\predictiva\\LAB-01-prediccion-del-default-usando-rf-jdholguinm\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\predictiva\\LAB-01-prediccion-del-default-usando-rf-jdholguinm\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:955\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    952\u001b[39m estimator = \u001b[38;5;28mself\u001b[39m.estimator\n\u001b[32m    953\u001b[39m scorers, refit_metric = \u001b[38;5;28mself\u001b[39m._get_scorers()\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m X, y = \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    958\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\predictiva\\LAB-01-prediccion-del-default-usando-rf-jdholguinm\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:530\u001b[39m, in \u001b[36mindexable\u001b[39m\u001b[34m(*iterables)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[32m    501\u001b[39m \n\u001b[32m    502\u001b[39m \u001b[33;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m \u001b[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    529\u001b[39m result = [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\predictiva\\LAB-01-prediccion-del-default-usando-rf-jdholguinm\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [20953, 8979]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c34f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\predictiva\\\\LAB-01-prediccion-del-default-usando-rf-jdholguinm\\\\homework'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.path.getsize(\"../files/models/model.pkl.gz\")\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
